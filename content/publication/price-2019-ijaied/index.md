---
# Documentation: https://wowchemy.com/docs/managing-content/

title: A Comparison of the Quality of Data-driven Programming Hint Generation Algorithms
subtitle: ''
summary: ''
authors:
- Thomas W Price
- Yihuan Dong
- Rui Zhi
- Benjamin Paaßen
- Nicholas Lytle
- Veronica Cateté
- Tiffany Barnes
tags:
- Peer Reviewed Journal Article
- data-driven hints
- hint quality
- intelligent tutoring systems
- programming
categories:
- isnap
- hint-gen
date: '2019-01-01'
lastmod: 2021-07-07T11:39:42-04:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2021-07-07T15:39:41.411314Z'
publication_types:
- '2'
abstract: In the domain of programming, a growing number of algorithms automatically
  generate data-driven, next-step hints that suggest how students should edit their
  code to resolve errors and make progress. While these hints have the potential to
  improve learning if done well, few evaluations have directly assessed or compared
  the quality of different hint generation approaches. In this work, we present the
  QualityScore procedure, a novel method for automatically evaluating and comparing
  the quality of next-step programming hints. We first demonstrate that the automated
  QualityScore ratings agree with experts' manual ratings. We then use the QualityScore
  procedure to compare the quality of 6 data-driven, next-step hint generation algorithms
  using two distinct programming datasets in two different programming languages.
  Our results show that there are large and significant differences between the quality
  of the 6 algorithms and that these differences are relatively consistent across
  datasets and problems. We also identify situations where the six algorithms struggle
  to produce high-quality hints, and we suggest ways that future work might address
  these gaps in quality hint coverage.
publication: '*International Journal of Artificial Intelligence in Education*'
---
